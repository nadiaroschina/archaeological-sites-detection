{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68749399-e209-4031-ba98-11be37c6fd10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa4d7f-0d90-4b37-b9d2-8dc4ece2dd2b",
   "metadata": {},
   "source": [
    "## Данные и модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df28da3-1a4b-48cf-b11f-0438e055edc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset import ArchDataset, GS_NEGATIVE_PATHS, GS_POSITIVE_PATHS, GS_NEGATIVE_COORDS, GS_POSITIVE_COORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ff7b3-30d1-4716-ad07-8262ded43499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import VQVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947bcfd-e308-4dbc-a969-753c9c437b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Параметры модели и обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284d3a2-5960-4569-bca7-280d62b13e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "NUM_EMBEDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f4d68-bdca-4181-aa62-fe29b4685e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8814b-e41d-42f8-a335-3641e44911fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dir = f'../vq_vae_training_sasgis_results/v2_256pix_gray_{str(EMBED_DIM).zfill(2)}embdim_{str(NUM_EMBEDS).zfill(2)}embeds'\n",
    "res_img_path = f'v2_256pix_gray_{str(EMBED_DIM).zfill(2)}embdim_{str(NUM_EMBEDS).zfill(2)}embeds'\n",
    "checkpoint_path = f'{results_dir}/checkpoint_9.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b37cd-cf73-4402-aae4-c04f0c9e3367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_scores_dir = f'results_basevit_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f15cc-4f1b-4e34-acce-3c9f9e317d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'img_size': 256,\n",
    "    'channels': 1,\n",
    "    'embedding_dim': EMBED_DIM,\n",
    "    'num_embeddings': NUM_EMBEDS,\n",
    "    'beta': 0.25,\n",
    "    'n_epochs': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269588e-5f6c-4e91-a4f2-e0da1f1e3e86",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdc949-0d1e-4b73-a40b-8de1941eacd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_nocrop_256_grayscale = transforms.Compose([\n",
    "    # transforms.CenterCrop((256, 256)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa3304-7bb2-441b-8a99-8163f0d94bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97664d61-df89-41b6-b398-961a23072187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ArchDataset(\n",
    "    img_paths=GS_NEGATIVE_PATHS, \n",
    "    coords=GS_NEGATIVE_COORDS, \n",
    "    anomalies=False, \n",
    "    transform=transform_nocrop_256_grayscale\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "anomalies_dataset = ArchDataset(\n",
    "    img_paths=GS_POSITIVE_PATHS, \n",
    "    coords=GS_POSITIVE_COORDS, \n",
    "    anomalies=True, \n",
    "    transform=transform_nocrop_256_grayscale\n",
    ")\n",
    "anomalies_loader = DataLoader(anomalies_dataset, batch_size=params['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e7a86-873c-4aea-996b-85bb719bc4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'train: {len(train_dataset)} images, {len(train_loader)} batches')\n",
    "print(f'anomalies: {len(anomalies_dataset)} images, {len(anomalies_loader)} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ba0c1-d3c2-4ee0-997a-d82fe566d539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncols = 6\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=ncols, figsize=(16, 8))\n",
    "for i in range(ncols):\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].axis('off')\n",
    "    axs[0, i].imshow(anomalies_dataset[i][0].permute(1, 2, 0), cmap='gray')\n",
    "    axs[1, i].imshow(train_dataset[i][0].permute(1, 2, 0), cmap='gray')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b8f23-3bc0-4a5d-9878-1c547d0d73eb",
   "metadata": {},
   "source": [
    "# Модель, восстанавливающая изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f276f-68b2-48a4-b502-7b8dcbb786be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VQVAE(\n",
    "    in_channels=params['channels'],\n",
    "    img_size=params['img_size'],\n",
    "    \n",
    "    embedding_dim=params['embedding_dim'],\n",
    "    num_embeddings=params['num_embeddings'],\n",
    "    beta=params['beta']\n",
    ")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bd582-dd4a-4b53-be4f-1a0cd04ce548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a47391f2-b929-4c36-a452-3f335d4019fa",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21fb37-e648-4576-9968-d032e69882b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(names):\n",
    "    imgs = []\n",
    "    for name in names:\n",
    "        img = transform_nocrop_256_grayscale(Image.open(name).convert('RGB'))\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "def align_image(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def draw_image(imgs):\n",
    "    fig, axes = plt.subplots(ncols=len(imgs) , figsize=(len(imgs) * 2, 2))\n",
    "\n",
    "    if len(imgs) == 1:\n",
    "        img = imgs[0]\n",
    "        axes.imshow(align_image(img).permute(1, 2, 0))\n",
    "    else:\n",
    "        for i, img in enumerate(imgs): \n",
    "            axes[i].imshow(align_image(img).permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b19b18-a8fb-47ed-8a75-f99b35303498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edf88b36-75c5-408e-926e-2c473335fc8b",
   "metadata": {},
   "source": [
    "# Вычисление anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c619e2-448d-4828-8c9d-055ac0e30fff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "\n",
    "vit_model = ViTForImageClassification.from_pretrained(MODEL_NAME)\n",
    "vit_model.classifier = nn.Identity()\n",
    "vit_model.to(device)\n",
    "vit_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be3527-894f-4c5d-86cd-c846f0717b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vit_input_processor = ViTFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "vit_input_processor = transforms.Compose([\n",
    "    transforms.Resize(224, antialias=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5738a17-a009-4ae8-8c4b-ca3a456ae714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(imgs):\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    inputs = torch.stack([vit_input_processor(img) for img in imgs])\n",
    "    inputs = torch.cat([inputs, inputs, inputs], dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(inputs)\n",
    "\n",
    "    features = outputs.logits\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662977f3-0383-420c-a0a1-7923e9700675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(data_loader, limit_batches=None):\n",
    "\n",
    "    limit_batches = limit_batches if limit_batches is not None else len(data_loader)\n",
    "    anomaly_scores_res = {}\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(data_loader), total=limit_batches):\n",
    "        \n",
    "        if limit_batches and batch_idx >= limit_batches:\n",
    "            break\n",
    "        \n",
    "        images, indices, _, _ = batch\n",
    "        images = images.to(device)\n",
    "        indices = indices.cpu().tolist()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            restored_images, _, _ = model(images)\n",
    "\n",
    "        feat_original = extract_features(images)\n",
    "        feat_restored = extract_features(restored_images)\n",
    "        \n",
    "        cosine_sim = F.cosine_similarity(feat_original, feat_restored, dim=1)\n",
    "        anomaly_scores_batch = (1 - cosine_sim).cpu().tolist()\n",
    "        \n",
    "        for (idx, score) in zip(indices, anomaly_scores_batch):\n",
    "            anomaly_scores_res[idx] = score\n",
    "            \n",
    "    \n",
    "    return anomaly_scores_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6201b-dd8e-4018-8309-6898a533a950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_scores = compute_anomaly_scores(anomalies_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104e0e3-cba7-46b4-96eb-e6dafc17ae0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_scores = compute_anomaly_scores(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6d6f4-cbe2-4ae8-b039-03b38f8abb58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'{anomaly_scores_dir}/anomalies.txt', 'w') as f:\n",
    "    for idx, score in anomaly_scores.items():\n",
    "        f.write(f\"{idx}\\t{score}\\n\")\n",
    "        \n",
    "with open(f'{anomaly_scores_dir}/train.txt', 'w') as f:\n",
    "    for idx, score in train_scores.items():\n",
    "        f.write(f\"{idx}\\t{score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9710d2f-2f40-4636-b8f1-8bbb17f75082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Считываем и смотрим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240121d-15c5-49b1-80c4-d34bee3c91bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_scores = {}\n",
    "with open(f'{anomaly_scores_dir}/anomalies.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        idx, score = line.strip().split()\n",
    "        anomaly_scores[int(idx)] = float(score)\n",
    "        \n",
    "train_scores = {}\n",
    "with open(f'{anomaly_scores_dir}/train.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        idx, score = line.strip().split()\n",
    "        train_scores[int(idx)] = float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663eed2d-08a4-4dcb-8fd2-1274af40447a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(train_scores.values(), density=True, bins=50, alpha=0.4, label='train', color='cornflowerblue');\n",
    "plt.hist(anomaly_scores.values(), density=True, bins=50, alpha=0.4, label='anomaly', color='hotpink');\n",
    "plt.legend();\n",
    "plt.savefig('basevit_v1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63388de3-1310-4205-b323-a7f2aacee8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_scores_np = np.array(sorted(anomaly_scores.values()))\n",
    "train_scores_np = np.array(sorted(train_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c70f0d-ba7d-4a58-9315-2f59503e7315",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62131459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_scores_np = np.array(sorted(anomaly_scores.values()))\n",
    "train_scores_np = np.array(sorted(train_scores.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe96ed-063f-4266-8633-1572ffc65eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = np.array([1] * len(anomaly_scores_np) + [0] * len(train_scores_np))\n",
    "y_scores = np.concatenate((anomaly_scores_np, train_scores_np), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96338912-c349-4562-ad03-8187d9838e50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "plt.savefig('basevit_v1_roc');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea5cdf-5472-4405-b648-a933f7fbff15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_thresholds = np.array([0.80, 0.85, 0.90, 0.95])\n",
    "thesholds = np.quantile(anomaly_scores_np, 1 - precision_thresholds)\n",
    "percent_disproved = np.array([\n",
    "    np.mean(train_scores_np < threshold)\n",
    "    for threshold in thesholds\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc551cc-a20e-4536-a8e9-5e467d7a4750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# сколько процентов данных отбрасываются из рассмотрения для фиксированного порого точности\n",
    "\n",
    "for (pt, ast, pd) in zip(precision_thresholds, thesholds, percent_disproved):\n",
    "    print(f'precision {pt:.02f} anomaly score threshold {ast:.02f} result {pd * 100:.02f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316ebcb-81fe-4b8d-9c3e-f02d1c15e43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
